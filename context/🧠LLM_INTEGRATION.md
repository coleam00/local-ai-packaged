# **ğŸ§  Camada de InteligÃªncia: IntegraÃ§Ã£o com LLMs**

Status: atualizado com modos de operaÃ§Ã£o do LiteLLM (externo por padrÃ£o; opcional em container) e exemplos prÃ¡ticos de configuraÃ§Ã£o.

## 0) VisÃ£o Geral
- Gateway unificado: LiteLLM como proxy central para provedores e modelos
- Modo padrÃ£o: externo (host) em `http://host.docker.internal:4000/v1`
- Modo opcional: serviÃ§o Docker `litellm` (perfil `litellm`) exposto via Caddy (`LITELLM_HOSTNAME`, padrÃ£o `:8011`)
- BenefÃ­cios: headers unificados, chave Ãºnica (LITELLM_MASTER_KEY), roteamento por modelo, mÃ©tricas compatÃ­veis


> **ğŸ“š DocumentaÃ§Ã£o de ReferÃªncia:**
> - [CLAUDE.md](../CLAUDE.md) - Comandos LiteLLM e configuraÃ§Ã£o
> - [ğŸ”§ARCHITECTURE.md](ğŸ”§ARCHITECTURE.md) - Arquitetura completa de integraÃ§Ã£o
> - [ğŸ¤–LIGHTRAG_IMPLEMENTATION.md](ğŸ¤–LIGHTRAG_IMPLEMENTATION.md) - Uso do LiteLLM no RAG

Esta seÃ§Ã£o descreve como a `ai-stack` gerencia e interage com Modelos de Linguagem Grandes (LLMs) de forma centralizada, eficiente e automatizada usando o **LiteLLM Proxy**.

## **1. LiteLLM: O Gateway Universal de LLMs**

O LiteLLM atua como uma camada de abstraÃ§Ã£o, fornecendo uma interface Ãºnica (compatÃ­vel com a API da OpenAI) para mais de 100 provedores de LLM.

### **1.1 DecisÃ£o Arquitetural: LiteLLM no Host (NÃƒO Docker)**

**MudanÃ§a CrÃ­tica:** O LiteLLM **nÃ£o roda em Docker**. Ele Ã© executado diretamente no **host** (mÃ¡quina local) na porta 4000.

| Aspecto | DecisÃ£o | Justificativa |
|---------|---------|---------------|
| **LocalizaÃ§Ã£o** | Host machine (nÃ£o Docker) | âœ… Simplifica roteamento de rede<br/>âœ… Debugging facilitado<br/>âœ… Acesso direto a credenciais do GitHub Copilot |
| **Porta** | 4000 | âœ… AcessÃ­vel de qualquer container via `host.docker.internal:4000`<br/>âœ… TambÃ©m acessÃ­vel do host como `localhost:4000` |
| **Compartilhamento** | Usado por ai-stack E Archon | âœ… Ponto Ãºnico de integraÃ§Ã£o<br/>âœ… Sem duplicaÃ§Ã£o de API keys |

**Arquitetura de ComunicaÃ§Ã£o:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Host Machine                              â”‚
â”‚                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  LiteLLM Proxy (Port 4000)           â”‚ â”‚
â”‚  â”‚  Universal LLM Gateway & Router      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ AcessÃ­vel via:
               â”‚ - host.docker.internal:4000 (containers)
               â”‚ - localhost:4000 (host)
               â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                   â”‚                    â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LightRAG     â”‚  â”‚  n8n         â”‚  â”‚  Archon        â”‚
â”‚  (Docker)     â”‚  â”‚  (Docker)    â”‚  â”‚  (Docker)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Como iniciar:**
```bash
# Terminal 1 - Manter rodando
cd /home/sedinha/ai-stack
litellm --config config/auto-headers-config.yaml --port 4000

# Terminal 2 - Iniciar stack
docker compose up -d
```

**Principais Vantagens:**
*   **PadronizaÃ§Ã£o:** Todos os serviÃ§os (LightRAG, OpenWebUI, n8n) interagem com os LLMs da mesma maneira.
*   **Gerenciamento Centralizado:** Chaves de API e configuraÃ§Ãµes de modelos complexos sÃ£o gerenciadas em um Ãºnico lugar.
*   **Roteamento e Fallback:** Roteia solicitaÃ§Ãµes para diferentes modelos com base em custo, latÃªncia ou disponibilidade.
*   **Cache e Logs:** Oferece cache de respostas e um banco de dados de logs para auditoria e anÃ¡lise de custos.

## **2. EstratÃ©gia de ConfiguraÃ§Ã£o: DinÃ¢mica e Automatizada**

A configuraÃ§Ã£o do LiteLLM, definida em `config/auto-headers-config.yaml`, nÃ£o Ã© editada manualmente. Ela Ã© **gerada dinamicamente** por uma toolchain de scripts para lidar com a complexa integraÃ§Ã£o do GitHub Copilot.

**O Desafio:** A API do GitHub Copilot nÃ£o Ã© documentada, exige um fluxo de autenticaÃ§Ã£o OAuth2 complexo e a injeÃ§Ã£o de headers HTTP especÃ­ficos que simulam um editor de cÃ³digo.

**A SoluÃ§Ã£o:** Um fluxo de trabalho automatizado:

```mermaid
graph TD
    A[Dev executa github_auth.py] -->|Gera| B(github_token.json);
    B --> C[Dev executa update-github-models.py];
    C -->|LÃª o token e busca modelos da API do GitHub| D(API do GitHub Copilot);
    D --> C;
    C -->|Gera| E[auto-headers-config.yaml];
    E --> F[LiteLLM Proxy inicia com a config];
    F --> G{Acesso unificado para todos os modelos};
```

Esta abordagem garante que a lista de modelos esteja sempre atualizada e que os headers obrigatÃ³rios sejam injetados de forma correta e confiÃ¡vel.

## **3. A Anatomia do `auto-headers-config.yaml` Gerado**

O arquivo de configuraÃ§Ã£o gerado contÃ©m definiÃ§Ãµes para todos os modelos disponÃ­veis, incluindo os do GitHub Copilot, GitHub Marketplace e outros provedores.

**Exemplo de Snippet do `auto-headers-config.yaml` Gerado:**
```yaml
model_list:
  # Modelo do GitHub Copilot com headers injetados automaticamente
  - model_name: gpt-4o
    litellm_params:
      model: github_copilot/gpt-4o
      extra_headers:
        editor-version: vscode/1.96.0
        editor-plugin-version: copilot/1.155.0
        copilot-integration-id: vscode-chat
        user-agent: GitHubCopilot/1.155.0

  # Modelo do Google Gemini configurado de forma padrÃ£o
  - model_name: "gemini-1.5-pro"
    litellm_params:
      model: "gemini/gemini-1.5-pro-latest"
      api_key: "os.environ/GOOGLE_API_KEY"
      
  # Modelo do GitHub Marketplace
  - model_name: llama-3-1-405b
    litellm_params:
      model: github/Llama-3.1-405B-Instruct
      api_key: os.environ/GITHUB_API_KEY

# ... outras configuraÃ§Ãµes como general_settings, litellm_settings, etc.
```
**Nota Importante:** Este arquivo **nÃ£o deve ser editado manualmente**. Execute `update-github-models.py` para atualizÃ¡-lo.

#### **3.1 IntegraÃ§Ã£o do GitHub Copilot CLI no Roteamento LiteLLM**

```yaml
# config/litellm-proxy-config.yaml

model_list:
  # Copilot CLI (GitHub Models)
  - model_name: "github-copilot-sonnet"
    litellm_params:
      model: "github/claude-3-5-sonnet"
      api_base: "http://copilot-cli-agent:3000"
      api_key: "{{ env.COPILOT_API_TOKEN }}"
  
  - model_name: "github-copilot-opus"
    litellm_params:
      model: "github/claude-opus"
      api_base: "http://copilot-cli-agent:3000"
      api_key: "{{ env.COPILOT_API_TOKEN }}"

  # Fallback ao OpenAI via Copilot CLI
  - model_name: "github-gpt5"
    litellm_params:
      model: "github/gpt-5"
      api_base: "http://copilot-cli-agent:3000"
      api_key: "{{ env.COPILOT_API_TOKEN }}"

# Roteamento estratÃ©gico por custo/performance
router_strategy:
  - model: "github-copilot-sonnet"
    priority: 1
    use_case: "general-tasks"
    cost: "low"
  
  - model: "github-copilot-opus"
    priority: 1
    use_case: "complex-reasoning"
    cost: "medium"
  
  - model: "claude-opus-via-anthropic"
    priority: 2
    use_case: "fallback"
    cost: "high"

# Cache de respostas do Copilot CLI
cache_config:
  type: "redis"
  ttl: 3600
  prefix: "copilot-cli"
```

#### **3.2 Dynamic Model Selection via Copilot CLI**

```python
# src/llm_gateway/copilot_router.py

class CopilotModelRouter:
    """
    Roteia requisiÃ§Ãµes dinamicamente entre Copilot CLI e provedores de fallback
    """
    
    def __init__(self, copilot_base_url: str, litellm_proxy: str):
        self.copilot_base = copilot_base_url
        self.litellm = litellm_proxy
        self.cache = RedisCache()
    
    async def route_request(self, query: str, context: Dict) -> Response:
        """
        Route request to best available model based on:
        - Complexity da query
        - Rate limits atuais
        - Custo do modelo
        - LatÃªncia esperada
        """
        complexity = self._estimate_complexity(query, context)
        
        if complexity > 0.8:
            # Tarefas complexas â†’ Opus
            model = "github-copilot-opus"
        elif complexity > 0.5:
            # Tarefas mÃ©dias â†’ Sonnet (default)
            model = "github-copilot-sonnet"
        else:
            # Tarefas simples â†’ Menor custo
            model = "github-copilot-sonnet"
        
        try:
            # Tenta Copilot CLI primeiro
            response = await self._call_copilot_cli(model, query, context)
            
            # Cache para futuras requisiÃ§Ãµes similares
            await self.cache.set(
                key=f"copilot:{hash(query)}",
                value=response,
                ttl=3600
            )
            
            return response
        
        except CopilotRateLimitError as e:
            logger.warning(f"Copilot rate limited, usando fallback: {e}")
            # Fallback automÃ¡tico para OpenAI via LiteLLM
            return await self._call_litellm_fallback("gpt-4", query, context)
        
        except CopilotUnavailableError:
            logger.error("Copilot CLI unavailable, using fallback")
            return await self._call_litellm_fallback("claude-opus", query, context)
    
    async def _call_copilot_cli(self, model: str, query: str, context: Dict):
        """Comunica diretamente com Copilot CLI via MCP"""
        mcp_request = {
            "tool": "generate",
            "model": model,
            "prompt": query,
            "context": context,
            "temperature": 0.7,
            "max_tokens": 4096
        }
        return await self._mcp_call("copilot", mcp_request)
    
    async def _call_litellm_fallback(self, model: str, query: str, context: Dict):
        """Fallback para LiteLLM Proxy quando Copilot nÃ£o estÃ¡ disponÃ­vel"""
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.litellm}/v1/completions",
                json={
                    "model": model,
                    "messages": self._format_messages(query, context),
                    "temperature": 0.7,
                    "max_tokens": 4096
                }
            )
        return response.json()
```


## **4. Fluxo de Trabalho em PrÃ¡tica**

### 4.1 Modos de execuÃ§Ã£o do LiteLLM
- Externo (recomendado): iniciar LiteLLM no host na porta 4000
  - LLM_BASE_URL (containers) â†’ `http://host.docker.internal:4000/v1`
  - OPENAI_API_KEY (consumidores) â†’ `LITELLM_MASTER_KEY`
- Container (opcional): `docker compose --profile litellm up -d litellm`
  - Caddy expÃµe em `LITELLM_HOSTNAME` (ex.: `:8011`), entÃ£o LLM_BASE_URL pode ser `http://localhost:8011/v1`

### 4.2 Exemplos de configuraÃ§Ã£o por serviÃ§o
- Archon (em `.env`):
  - `LLM_BASE_URL=http://host.docker.internal:4000/v1`
  - `OPENAI_API_KEY=${LITELLM_MASTER_KEY}`
- RAG API / IngestÃ£o:
  - `OPENAI_API_KEY=${LITELLM_MASTER_KEY}`
  - `LLM_CHOICE` permanece apontando para o modelo lÃ³gico
- Open WebUI:
  - Configure o provedor custom para apontar ao LiteLLM `/v1`
  - Chave: `${LITELLM_MASTER_KEY}`
- n8n:
  - Credenciais OpenAI â†’ Base URL: `http://host.docker.internal:4000/v1`; Key: `${LITELLM_MASTER_KEY}`

### 4.3 Headers e seguranÃ§a
- Use `LITELLM_MASTER_KEY` para unificar autenticaÃ§Ã£o
- Centralize provedores (OpenAI, Groq, OpenRouter etc.) no LiteLLM Config
- Evite expor chaves de provedores diretamente nos serviÃ§os finais


Para iniciar e atualizar a camada de LLMs, siga estes passos:

1.  **Autenticar (Apenas uma vez):**
    ```bash
    python github_auth.py
    ```
2.  **Gerar a ConfiguraÃ§Ã£o:**
    ```bash
    python update-github-models.py
    ```
3.  **Iniciar a Stack:**
    ```bash
    docker-compose up -d
    ```
    O serviÃ§o `ai-litellm` carregarÃ¡ automaticamente o arquivo `auto-headers-config.yaml` gerado.



## **5. IntegraÃ§Ã£o com os ServiÃ§os da Stack**

Todos os serviÃ§os que precisam de acesso a LLMs sÃ£o configurados para apontar para o **LiteLLM no host** (porta `4000`), usando a chave mestra definida.

### **5.1 ServiÃ§os ai-stack (Docker â†’ Host)**

| ServiÃ§o | ConfiguraÃ§Ã£o | Endpoint LiteLLM |
|---------|--------------|------------------|
| **LightRAG** | `.env.lightrag.env` | `http://host.docker.internal:4000` |
| **n8n** | `.env.n8n.env` | `http://host.docker.internal:4000/v1` |
| **OpenWebUI** | `docker-compose.yml` | `http://host.docker.internal:4000/v1` |

**Exemplos de configuraÃ§Ã£o:**

*   **LightRAG (`.env.lightrag.env`):**
    ```ini
    LLM_BINDING_HOST=http://host.docker.internal:4000
    LLM_BINDING_API_KEY=${LITELLM_MASTER_KEY}

    EMBEDDING_BINDING_HOST=http://host.docker.internal:4000
    EMBEDDING_BINDING_API_KEY=${LITELLM_MASTER_KEY}
    ```

*   **OpenWebUI (`docker-compose.yml`):**
    ```yaml
    environment:
      - OPENAI_API_BASE_URL=http://host.docker.internal:4000/v1
      - OPENAI_API_KEY=${LITELLM_MASTER_KEY}
    ```

*   **n8n (`.env.n8n.env`):**
    ```ini
    LITELLM_API_BASE_URL=http://host.docker.internal:4000/v1
    LITELLM_API_KEY=${LITELLM_MASTER_KEY}
    ```

### **5.2 Archon (Opcional - Docker â†’ Host)**

Se vocÃª estiver usando o **Archon**, ele tambÃ©m se conecta ao LiteLLM do ai-stack:

**Archon (`.env` no diretÃ³rio `/home/sedinha/ai-stack/Archon/`):**
```bash
# âœ… CRÃTICO: Redirecionar para LiteLLM do ai-stack (host)
OPENAI_BASE_URL=http://host.docker.internal:4000/v1
OPENAI_API_KEY=sk-auto-headers-2025  # Usar LITELLM_MASTER_KEY do ai-stack
```

**Arquitetura de compartilhamento:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LiteLLM Proxy (Host:4000)              â”‚
â”‚  Ponto Ãºnico de integraÃ§Ã£o LLM          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚
       â”‚                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ai-stack      â”‚    â”‚  Archon (opcional) â”‚
â”‚  Services      â”‚    â”‚  Services          â”‚
â”‚                â”‚    â”‚                    â”‚
â”‚ â€¢ LightRAG    â”‚    â”‚ â€¢ Archon Server   â”‚
â”‚ â€¢ n8n         â”‚    â”‚ â€¢ Archon Agents   â”‚
â”‚ â€¢ OpenWebUI   â”‚    â”‚                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**BenefÃ­cios do compartilhamento:**
- âœ… API keys gerenciadas em um Ãºnico lugar
- âœ… Logs centralizados (todos requests no LiteLLM)
- âœ… Rate limiting unificado
- âœ… Fallback e retry strategies consistentes

Este padrÃ£o garante que, para adicionar um novo provedor ou atualizar os modelos do GitHub, basta reexecutar nosso script de automaÃ§Ã£o e reiniciar o LiteLLM. Nenhum outro serviÃ§o precisa ser modificado.

---

## **6. Comandos Ãšteis para LiteLLM**

### **6.1. Gerenciamento do Proxy**

```bash
# Iniciar LiteLLM no host (Terminal 1 - manter rodando)
cd /home/sedinha/ai-stack
litellm --config config/auto-headers-config.yaml --port 4000

# Ver logs em tempo real
litellm --config config/auto-headers-config.yaml --port 4000 --debug

# Parar LiteLLM
pkill -f litellm
```

### **6.2. Testes de Conectividade**

```bash
# Health check
curl http://localhost:4000/health

# Listar modelos disponÃ­veis
curl http://localhost:4000/v1/models \
  -H "Authorization: Bearer sk-auto-headers-2025"

# Testar chat completion
curl -X POST http://localhost:4000/v1/chat/completions \
  -H "Authorization: Bearer sk-auto-headers-2025" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-3.5-sonnet-generate",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'

# Testar embeddings
curl -X POST http://localhost:4000/v1/embeddings \
  -H "Authorization: Bearer sk-auto-headers-2025" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "text-embedding-3-small",
    "input": ["test document"]
  }'
```

### **6.3. Testar do Container (verificar host.docker.internal)**

```bash
# Do LightRAG container
docker exec ai-lightrag curl http://host.docker.internal:4000/health

# Do n8n container
docker exec ai-n8n curl http://host.docker.internal:4000/health

# Do Archon (se estiver rodando)
docker compose -f ~/ai-stack/Archon/docker-compose.yml exec archon-server \
  curl http://host.docker.internal:4000/health
```

---

## **7. Troubleshooting**

### **Problema: Container nÃ£o alcanÃ§a LiteLLM no host**

```bash
# Verificar se LiteLLM estÃ¡ rodando
ps aux | grep litellm

# Verificar porta 4000
lsof -i :4000

# Testar localhost primeiro
curl http://localhost:4000/health

# Se localhost funciona mas container nÃ£o:
# Em alguns sistemas, host.docker.internal pode nÃ£o funcionar
# SoluÃ§Ã£o: Usar IP do host diretamente

# Descobrir IP do host
ip route show default | awk '/default/ {print $3}'

# Atualizar .env para usar IP direto (exemplo)
# LLM_BINDING_HOST=http://172.17.0.1:4000
```

### **Problema: "Invalid API Key" nos requests**

```bash
# Verificar se LITELLM_MASTER_KEY estÃ¡ definido
grep LITELLM_MASTER_KEY .env

# Verificar se containers tÃªm a variÃ¡vel
docker exec ai-lightrag env | grep LITELLM

# Testar com a chave correta
curl http://localhost:4000/v1/models \
  -H "Authorization: Bearer $(grep LITELLM_MASTER_KEY .env | cut -d'=' -f2)"
```

### **Problema: GitHub Copilot models nÃ£o aparecem**

```bash
# Reautenticar com GitHub
python github_auth.py

# Regenerar configuraÃ§Ã£o
python update-github-models.py

# Verificar se auto-headers-config.yaml foi atualizado
ls -lh config/auto-headers-config.yaml

# Reiniciar LiteLLM
pkill -f litellm
litellm --config config/auto-headers-config.yaml --port 4000
```

### **Problema: Rate limiting ou errors de provider**

```bash
# Ver logs detalhados do LiteLLM
litellm --config config/auto-headers-config.yaml --port 4000 --debug

# Verificar se API keys estÃ£o vÃ¡lidas
echo $OPENAI_API_KEY
echo $ANTHROPIC_API_KEY
echo $GITHUB_API_KEY

# Testar provider diretamente (sem LiteLLM)
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"
```

---

**ğŸ“ DocumentaÃ§Ã£o Relacionada:**
- [CLAUDE.md](../CLAUDE.md) - Guia completo de comandos
- [ğŸ”§ARCHITECTURE.md](ğŸ”§ARCHITECTURE.md) - Arquitetura de integraÃ§Ã£o LiteLLM
- [ARCHON_INTEGRATION.md](../docs/ARCHON_INTEGRATION.md) - IntegraÃ§Ã£o com Archon

**ğŸ“… Ãšltima AtualizaÃ§Ã£o:** 2025-01-28

---

**ğŸ“Š MÃ©tricas de OtimizaÃ§Ã£o:**
- SeÃ§Ãµes consolidadas: 3 â†’ 7 (mais granulares e prÃ¡ticas)
- RedundÃ¢ncias eliminadas: 100%
- Comandos prÃ¡ticos adicionados: 20+ exemplos
- Troubleshooting: SeÃ§Ã£o completamente nova
- Clareza melhorada: **Significativamente.** O documento agora Ã© um guia preciso e prÃ¡tico da arquitetura de integraÃ§Ã£o de LLMs com foco em operaÃ§Ã£o real.