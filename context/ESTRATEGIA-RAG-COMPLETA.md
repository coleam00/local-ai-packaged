# üöÄ ARQUITETURA ESTRAT√âGICA: ai-stack RAG + n8n + LiteLLM + MCP

## üìã VIS√ÉO GERAL DO PROJETO

Voc√™ est√° construindo um **sistema de IA modular e produ√ß√£o-grade** que integra:

1. **LiteLLM** (porta 4001) - Router de LLMs com 33+ modelos
2. **n8n** (porta 5678) - Orquestra√ß√£o de workflows visuais
3. **OpenWebUI** (porta 3001) - Interface cliente para usu√°rios finais
4. **LightRAG** (porta 9621) - Engine de RAG avan√ßado
5. **Neo4j** (porta 7474) - Banco de dados gr√°fico
6. **Supabase+pgVector** (porta 5432) - Vetores e dados estruturados
7. **Claude Code + MCP** - Interface de desenvolvimento program√°tico

### Fluxo de Dados Proposto

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          CLIENTE FINAL                          ‚îÇ
‚îÇ  (OpenWebUI, Claude Code, ou qualquer MCP client)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        n8n WORKFLOWS                            ‚îÇ
‚îÇ  (Orquestra√ß√£o, transforma√ß√£o, valida√ß√£o de dados)              ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  - Ingest√£o de documentos                                       ‚îÇ
‚îÇ  - Chunking e preprocessing                                     ‚îÇ
‚îÇ  - Enriquecimento com metadados                                 ‚îÇ
‚îÇ  - Gerenciamento de tasks                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ                 ‚îÇ
                    ‚ñº                 ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   LightRAG API   ‚îÇ  ‚îÇ  LiteLLM Proxy   ‚îÇ
        ‚îÇ   (9621)         ‚îÇ  ‚îÇ   (4001)         ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                                        ‚îÇ
        ‚ñº                                        ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Neo4j KG    ‚îÇ                    ‚îÇ  OpenAI,     ‚îÇ
    ‚îÇ  (7474)      ‚îÇ                    ‚îÇ  Anthropic,  ‚îÇ
    ‚îÇ              ‚îÇ                    ‚îÇ  Google      ‚îÇ
    ‚îÇ  Graphs &    ‚îÇ                    ‚îÇ              ‚îÇ
    ‚îÇ  Relations   ‚îÇ                    ‚îÇ  33+ models  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Supabase     ‚îÇ
    ‚îÇ + pgVector   ‚îÇ
    ‚îÇ (5432)       ‚îÇ
    ‚îÇ              ‚îÇ
    ‚îÇ Embeddings   ‚îÇ
    ‚îÇ & Metadata   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéØ ESTRAT√âGIA RAG MULTICAMADAS

### Camada 1: Ingest√£o (n8n Workflow)

**Responsabilidade:** Receber documentos e preparar para processamento

```yaml
Workflow: "RAG_Ingest√£o"
Triggers:
  - OpenWebUI (via HTTP)
  - MCP Client (Claude Code)
  - N8N HTTP Endpoint
  
Steps:
  1. Receber documento (PDF, TXT, URL)
  2. Validar formato e tamanho
  3. Extrair conte√∫do (OCR se PDF)
  4. Dividir em chunks (128-512 tokens)
  5. Adicionar metadados (source, date, tags)
  6. Enviar para LightRAG
```

### Camada 2: Processamento (LightRAG)

**Responsabilidade:** RAG inteligente com m√∫ltiplas estrat√©gias

**3 Estrat√©gias Paralelas:**

#### A. **Semantic Search** (Vectorial)
- Embeddings via LiteLLM (text-embedding-3-small)
- Armazenamento em Supabase pgVector
- Busca por similaridade sem√¢ntica
- **Melhor para:** Conceitos e temas relacionados

#### B. **Knowledge Graph** (Neo4j + Graphiti)
- Extra√ß√£o autom√°tica de entidades (pessoas, orgs, conceitos)
- Extra√ß√£o de relacionamentos (has_knowledge, belongs_to, related_to)
- Armazenamento em Neo4j
- Busca por caminho gr√°fico (path traversal)
- **Melhor para:** Contexto relacional complexo

#### C. **BM25 Search** (Full-text)
- Indexa√ß√£o full-text built-in no Graphiti
- Busca por keywords espec√≠ficas
- **Melhor para:** Busca exata e espec√≠fica

### Camada 3: Gera√ß√£o (LiteLLM)

**Responsabilidade:** Generar respostas contextualmente relevantes

```
Prompt Structure:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ SYSTEM PROMPT                               ‚îÇ
‚îÇ - Papel da IA                               ‚îÇ
‚îÇ - Instru√ß√µes de tom e estilo                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CONTEXTO RECUPERADO (RAG)                   ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ [Semantic Results]                          ‚îÇ
‚îÇ "Based on document X: ..."                  ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ [Graph Results]                             ‚îÇ
‚îÇ "Related entities: ..."                     ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ [BM25 Results]                              ‚îÇ
‚îÇ "Exact matches: ..."                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ USER QUERY                                  ‚îÇ
‚îÇ "Qual √© a rela√ß√£o entre X e Y?"             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LiteLLM ‚Üí Melhor Modelo Dispon√≠vel          ‚îÇ
‚îÇ (gpt-4o, claude-3.5-sonnet, gemini-2.0)     ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ Gera resposta sintetizada                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è COMPARA√á√ÉO: Graphiti vs GraphRAG vs Neo4j Vanilla

### Graphiti (RECOMENDADO para seu caso)

| Aspecto | Graphiti |
|---------|----------|
| **Tipo** | Framework de Knowledge Graph din√¢mico |
| **Atualiza√ß√£o** | Real-time incremental ‚úÖ |
| **Busca** | H√≠brida (semantic + BM25 + graph) ‚úÖ |
| **Temporal** | Bi-temporal (evento vs ingest√£o) ‚úÖ |
| **Neo4j Required?** | Sim (roda sobre Neo4j) |
| **MCP Server** | Integra√ß√£o nativa ‚úÖ |
| **Lat√™ncia** | Sub-segundo |
| **Customiza√ß√£o** | Entidades custom via Pydantic |
| **Melhor para** | Dados din√¢micos e relacionamentos complexos |

### GraphRAG (Microsoft)

| Aspecto | GraphRAG |
|---------|----------|
| **Tipo** | Batch-oriented document summarization |
| **Atualiza√ß√£o** | Reprocessamento completo ‚ùå |
| **Busca** | Comunidades + LLM summarization |
| **Temporal** | B√°sico |
| **Lat√™ncia** | Segundos a minutos |
| **Melhor para** | Documentos est√°ticos e grandes volumes |

### Neo4j Vanilla

| Aspecto | Neo4j |
|---------|-------|
| **Tipo** | Database de grafo |
| **Query** | Cypher (SQL-like) |
| **Busca** | Apenas graph traversal |
| **Voc√™ constr√≥i** | Toda a l√≥gica de RAG |
| **Melhor para** | Dados estruturados e queries complexas |

**Recomenda√ß√£o:** Use **Graphiti** + **Neo4j** + **Supabase**

---

## üîå APIS LITELLM PARA INGEST√ÉO RAG

### 1. Chat Completions (Sua API principal)

```bash
# Endpoint
POST /v1/chat/completions

# Request
{
  "model": "gpt-4o",
  "messages": [
    {
      "role": "system",
      "content": "Voc√™ √© um especialista em extra√ß√£o de conhecimento..."
    },
    {
      "role": "user",
      "content": "Extraia as entidades do texto: ..."
    }
  ],
  "temperature": 0.7,
  "max_tokens": 2000
}

# Response
{
  "choices": [{
    "message": {
      "content": "Entidades encontradas: ..."
    }
  }],
  "usage": {
    "prompt_tokens": 150,
    "completion_tokens": 450
  }
}
```

### 2. Embeddings (Para vetoriza√ß√£o)

```bash
# Endpoint
POST /v1/embeddings

# Request
{
  "model": "text-embedding-3-small",
  "input": ["Primeiro chunk de texto...", "Segundo chunk..."],
  "encoding_format": "float"
}

# Response
{
  "data": [
    {"embedding": [0.123, 0.456, ...], "index": 0},
    {"embedding": [0.789, 0.012, ...], "index": 1}
  ],
  "usage": {
    "prompt_tokens": 50,
    "total_tokens": 50
  }
}
```

### 3. Batch Processing (Para volumes grandes)

```bash
# Para ingerir 10.000 documentos em paralelo
POST /v1/chat/completions (com retry logic)

Usar concorr√™ncia com semaPhore:
- SEMAPHORE_LIMIT=50 (aumentar conforme sua rate limit)
```

### 4. Token Counting

```bash
# Verificar custos antes de processar
POST /v1/completions/token/count

{
  "messages": [...],
  "model": "gpt-4o"
}
```

---

## üîÑ FLUXO COMPLETO: OpenWebUI ‚Üí n8n ‚Üí LightRAG ‚Üí LiteLLM

### Step-by-Step

#### 1. Usu√°rio envia query no OpenWebUI

```
Usu√°rio: "Qual foi o impacto da lei X no setor Y?"
‚îÇ
‚îú‚îÄ‚Üí OpenWebUI registra a conversa em Supabase
‚îÇ
‚îî‚îÄ‚Üí HTTP POST para n8n webhook
```

#### 2. n8n recebe e processa

```yaml
n8n Workflow "RAG_Query_Handler":
  
  Step 1: Receber Query
    input: "Qual foi o impacto..."
  
  Step 2: Pr√©-processamento
    - Tokenizar query
    - Remover stopwords
    - Expandir com sin√¥nimos
  
  Step 3: Parallelizar 3 buscas
    
    Ramo A: Semantic Search
      - Embeddar query via LiteLLM
      - Buscar em Supabase pgVector
      - Top-5 resultados sem√¢nticos
    
    Ramo B: Graph Search
      - Enviar query para Neo4j
      - Buscar por entidades "lei X" e "setor Y"
      - Recuperar 10 hops de dist√¢ncia
    
    Ramo C: Full-text Search
      - Buscar keywords em LightRAG
      - Top-10 matches por relev√¢ncia
  
  Step 4: Consolidar contexto
    - Mesclar top-3 de cada ramo
    - Ordenar por relev√¢ncia
    - Limitar a 4K tokens
  
  Step 5: Chamar LiteLLM
    - Montar prompt final
    - Usar melhor modelo (gpt-4o)
    - Stream response
  
  Step 6: Retornar para OpenWebUI
    - Incluir fontes
    - Incluir confidence scores
    - Salvar para hist√≥rico
```

#### 3. LightRAG processa embedding

```python
# Pseudoc√≥digo
graphiti = Graphiti(
    graph_driver=Neo4jDriver(...),
    llm_client=OpenAIClient(config),
    embedder=OpenAIEmbedder(...)
)

# Ingerir documento
await graphiti.add_episodes(
    episodes=[{
        "content": "Lei X estabeleceu...",
        "source": "openwebui_user_123",
        "metadata": {"type": "user_query", "timestamp": "..."}
    }],
    **metadata
)

# Buscar contexto
results = await graphiti.search_relationships(
    query="impacto da lei X",
    limit=10,
    search_type="hybrid"  # semantic + keyword + graph
)
```

#### 4. LiteLLM gera resposta

```python
from litellm import completion

response = completion(
    model="gpt-4o",
    messages=[
        {
            "role": "system",
            "content": "Voc√™ √© um especialista em an√°lise de legisla√ß√£o..."
        },
        {
            "role": "user",
            "content": f"""
            Contexto de pesquisa:
            {consolidado_do_rag}
            
            Pergunta: Qual foi o impacto da lei X no setor Y?
            
            Responda em portugu√™s, citando as fontes.
            """
        }
    ],
    temperature=0.7,
    max_tokens=2000,
    stream=True
)

# Stream a resposta de volta pro OpenWebUI
for chunk in response:
    enviar_para_openwebui(chunk)
```

---

## üé® INTERFACE n8n + UI MELHORADA

### Op√ß√µes de UI:

#### 1. **n8n Built-in** (Mais simples)
- Use templates do n8n
- Custom HTML nodes
- Webhooks para renderizar UI externa

#### 2. **SuperDesign + n8n** (Recomendado)
- Criar UI em Figma ‚Üí SuperDesign
- Exportar como componentes React
- Integrar com n8n HTTP Node
- Dashboard para workflows

#### 3. **Next.js + n8n** (Mais flex√≠vel)
- Frontend React/Next.js separado
- Chamar n8n workflows via API
- UI totalmente customizada

### Exemplo: Dashboard do RAG em Next.js

```typescript
// components/RAGDashboard.tsx
export function RAGDashboard() {
  const [query, setQuery] = useState("");
  const [results, setResults] = useState([]);
  const [loading, setLoading] = useState(false);

  const handleSearch = async (q: string) => {
    setLoading(true);
    
    // Chamar n8n webhook
    const response = await fetch("/api/n8n/rag-query", {
      method: "POST",
      body: JSON.stringify({
        query: q,
        userId: user.id,
        timestamp: Date.now()
      })
    });

    // LiteLLM streaming response
    const reader = response.body.getReader();
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      
      const text = new TextDecoder().decode(value);
      setResults(prev => [...prev, text]);
    }
    
    setLoading(false);
  };

  return (
    <div className="rag-dashboard">
      <SearchInput onSearch={handleSearch} />
      <ResultsPanel results={results} loading={loading} />
      <KnowledgeGraphVisualization />
    </div>
  );
}
```

---

## üîó INTEGRA√á√ÉO: OpenWebUI ‚Üî n8n ‚Üî LightRAG

### 1. Registrar Webhook no OpenWebUI

```bash
# Em OpenWebUI Settings
Webhooks > Add
- Name: n8n RAG Handler
- URL: http://localhost:5678/webhook/rag-process
- Events: message.created, document.uploaded
- Auth: Bearer {N8N_WEBHOOK_SECRET}
```

### 2. n8n Webhook Receiver

```yaml
Trigger: Webhook
- Method: POST
- Path: /webhook/rag-process
- Accept: application/json

Body:
{
  "event": "message.created",
  "message": {...},
  "userId": "...",
  "conversationId": "..."
}
```

### 3. Salvar no LightRAG

```python
# n8n Python node (se usar)
import requests

data = {
    "content": body["message"]["content"],
    "source": f"openwebui_{body['userId']}",
    "metadata": {
        "conversation_id": body["conversationId"],
        "timestamp": time.time(),
        "model": body.get("model"),
        "tokens": count_tokens(body["message"]["content"])
    }
}

# Ingerir no LightRAG
response = requests.post(
    "http://localhost:9621/api/episodes",
    json=data,
    headers={"Authorization": f"Bearer {LIGHTRAG_API_KEY}"}
)
```

---

## üß† MCP PARA LIGHTRAG (FastMCP)

### Criar MCP Server para LightRAG

```python
# lightrag_mcp_server.py
from fastmcp import Server
from pydantic import BaseModel

server = Server("lightrag-mcp")

class SearchQuery(BaseModel):
    query: str
    search_type: str = "hybrid"
    limit: int = 5

@server.tool()
async def search_rag(query: SearchQuery) -> dict:
    """Search the LightRAG knowledge base"""
    graphiti = get_graphiti_instance()
    results = await graphiti.search_relationships(
        query=query.query,
        limit=query.limit,
        search_type=query.search_type
    )
    return {
        "results": results,
        "count": len(results)
    }

@server.tool()
async def add_to_rag(content: str, metadata: dict) -> dict:
    """Add content to LightRAG"""
    graphiti = get_graphiti_instance()
    episode = await graphiti.add_episodes(
        episodes=[{
            "content": content,
            "metadata": metadata
        }]
    )
    return {"status": "ingested", "episode_id": episode.id}

@server.resource("rag://knowledge-graph")
async def get_graph_stats() -> dict:
    """Get statistics about the knowledge graph"""
    graphiti = get_graphiti_instance()
    stats = await graphiti.get_graph_stats()
    return stats

if __name__ == "__main__":
    server.run()
```

### Usar no Claude Code

```
# .mcp.json
{
  "mcpServers": {
    "lightrag": {
      "command": "python",
      "args": ["lightrag_mcp_server.py"],
      "env": {
        "NEO4J_URI": "bolt://localhost:7687",
        "OPENAI_API_KEY": "..."
      }
    }
  }
}
```

### Agora Claude pode fazer:

Claude Code:
Use o MCP do LightRAG para buscar informa√ß√µes sobre a arquitetura de microservi√ßos

Claude vai:
1. Chamar `search_rag` com query "arquitetura de microservi√ßos"
2. Receber resultados do Neo4j + pgVector
3. Usar no contexto da sua an√°lise
4. Adicionar insights com `add_to_rag`

---

## üéØ MODULARIZA√á√ÉO: Extrair Plane ‚Üí n8n

### Workflow no n8n para Task Management

```yaml
Workflow: "Plane_TaskManager"

Triggers:
  - n8n HTTP
  - Schedule (cron)
  - Manual trigger

Steps:
  1. Listar tarefas de uma projeto/espa√ßo
  2. Associar com chunks de RAG
  3. Atualizar status baseado em progresso
  4. Criar relacionamentos no Neo4j
  5. Notificar via webhook

Example:
  trigger: "Document ingest√£o conclu√≠da"
  -> create_task("Review document X")
  -> tag_with("rag_processed", "pending_review")
  -> notify_team("New RAG document ready for review")
```

---

## üìä STACK FINAL RECOMENDADO

### Tecnologias por Camada

| Camada | Componente | Tecnologia | Porta | Responsabilidade |
|--------|-----------|------------|-------|-----------------|
| **UI** | Cliente | OpenWebUI | 3001 | Interface usu√°rio |
| **Cliente** | IDE | Claude Code | MCP | Desenvolvimento |
| **Orchestra√ß√£o** | Workflows | n8n | 5678 | Automa√ß√£o |
| **LLM** | Router | LiteLLM | 4001 | Roteamento de modelos |
| **RAG** | Engine | LightRAG | 9621 | Busca contextual |
| **Dados** | Grafo | Neo4j | 7474 | Conhecimento estruturado |
| **Dados** | Vetores | Supabase pgVector | 5432 | Embeddings |
| **Desenvolvimento** | MCP** | FastMCP | Custom | Integra√ß√£o program√°tica |

---

## üöÄ PR√ìXIMOS PASSOS

1. **Implementar Graphiti MCP Server** (Prioridade 1)
   - Criar FastMCP server para LightRAG
   - Testar com Claude Code

2. **Criar n8n Workflows** (Prioridade 2)
   - RAG Ingest√£o
   - RAG Query Handler
   - Plane Task Manager

3. **Integrar OpenWebUI ‚Üí n8n** (Prioridade 3)
   - Webhooks
   - Autentica√ß√£o
   - Streaming responses

4. **UI Melhorada** (Prioridade 4)
   - Dashboard em Next.js
   - Visualiza√ß√£o Neo4j
   - Componentes Figma

5. **Adicionar Outras Estrat√©gias RAG** (Prioridade 5)
   - Reranking com cross-encoders
   - Recursive retrieval
   - Hypothetical Document Embeddings

---

## ‚úÖ CHECKLIST IMPLEMENTA√á√ÉO

```
[ ] LiteLLM rodando com /v1/embeddings
[ ] Supabase pgVector configurado
[ ] Neo4j acess√≠vel e populado
[ ] Graphiti instalado e testado
[ ] n8n webhook receiver pronto
[ ] OpenWebUI conectado a n8n
[ ] MCP Server LightRAG criado
[ ] Claude Code integrado com MCP
[ ] Dashboard Next.js basic
[ ] Documenta√ß√£o completa
```

---

Pronto para come√ßar? Qual camada quer implementar primeiro? üöÄ