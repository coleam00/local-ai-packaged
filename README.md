# Pack d‚ÄôIA auto-h√©berg√©

**Pack d‚ÄôIA auto-h√©berg√©** est un mod√®le open-source bas√© sur Docker Compose qui vous permet de d√©ployer en quelques minutes un environnement complet de d√©veloppement Local AI et low-code. Il inclut :

- **Ollama** pour vos mod√®les LLM locaux  
- **Open WebUI** pour dialoguer avec vos agents n8n  
- **Supabase** pour la base de donn√©es, le vector store et l‚Äôauthentification  

Cette version, maintenue par Cole, apporte plusieurs am√©liorations et ajoute Supabase, Open WebUI, Flowise, Langfuse, SearXNG et Caddy. Les workflows RAG AI Agent pr√©sent√©s dans la vid√©o seront automatiquement import√©s dans votre instance n8n.

---

## Liens importants

- [Communaut√© Local AI](https://thinktank.ottomator.ai/c/local-ai/18) (oTTomator Think Tank)  
- [Tableau Kanban GitHub](https://github.com/users/coleam00/projects/2/views/1) pour le suivi des fonctionnalit√©s et corrections  
- [Local AI Starter Kit d‚Äôorigine](https://github.com/n8n-io/self-hosted-ai-starter-kit) par l‚Äô√©quipe n8n  
- T√©l√©chargez mon int√©gration N8N + Open WebUI [directement sur le site Open WebUI](https://openwebui.com/f/coleam/n8n_pipe/) (instructions ci-dessous)

![n8n.io - Capture d‚Äô√©cran](https://raw.githubusercontent.com/n8n-io/self-hosted-ai-starter-kit/main/assets/n8n-demo.gif)

Con√ßu par <https://github.com/n8n-io> et <https://github.com/coleam00>, ce pack combine la plateforme n8n auto-h√©berg√©e avec une liste d‚Äôoutils IA compatibles pour d√©marrer rapidement vos workflows IA.

### Ce qui est inclus

‚úÖ [**n8n auto-h√©berg√©**](https://n8n.io/) ‚Äì Plate-forme low-code avec plus de 400 int√©grations et composants IA  
‚úÖ [**Supabase**](https://supabase.com/) ‚Äì Base de donn√©es open-source en tant que service, tr√®s utilis√©e pour les agents IA  
‚úÖ [**Ollama**](https://ollama.com/) ‚Äì Plate-forme cross-platform pour installer et ex√©cuter des LLM locaux  
‚úÖ [**Open WebUI**](https://openwebui.com/) ‚Äì Interface ChatGPT-like pour interagir en priv√© avec vos mod√®les et agents n8n  
‚úÖ [**Flowise**](https://flowiseai.com/) ‚Äì Constructeur d‚Äôagents IA no/low-code, compl√©mentaire √† n8n  
‚úÖ [**Qdrant**](https://qdrant.tech/) ‚Äì Vector store haute performance avec API compl√®te  
‚úÖ [**SearXNG**](https://searxng.org/) ‚Äì M√©ta-moteur de recherche libre, sans suivi utilisateur  
‚úÖ [**Caddy**](https://caddyserver.com/) ‚Äì Gestion automatique du HTTPS/TLS pour vos domaines  
‚úÖ [**Langfuse**](https://langfuse.com/) ‚Äì Plate-forme d‚Äôobservabilit√© pour vos agents LLM  

---

## Pr√©requis

Avant de commencer, assurez-vous d‚Äôavoir install√© :

- [Python](https://www.python.org/downloads/) ‚Äì N√©cessaire pour lancer le script d‚Äôinstallation  
- [Git / GitHub Desktop](https://desktop.github.com/) ‚Äì Pour g√©rer le d√©p√¥t  
- [Docker / Docker Desktop](https://www.docker.com/products/docker-desktop/) ‚Äì Pour ex√©cuter tous les services  

---

## Installation

Clonez le d√©p√¥t et naviguez dans le dossier :

```bash
git clone https://github.com/coleam00/local-ai-packaged.git
cd local-ai-packaged

Avant de lancer les services, cr√©ez et √©ditez votre fichier d‚Äôenvironnement :

cp .env.example .env

Ouvrez .env et renseignez les variables suivantes :

############
# Configuration n8n
############
N8N_ENCRYPTION_KEY=
N8N_USER_MANAGEMENT_JWT_SECRET=

############
# Secrets Supabase
############
POSTGRES_PASSWORD=
JWT_SECRET=
ANON_KEY=
SERVICE_ROLE_KEY=
DASHBOARD_USERNAME=
DASHBOARD_PASSWORD=
POOLER_TENANT_ID=

############
# Identifiants Langfuse
############
CLICKHOUSE_PASSWORD=
MINIO_ROOT_PASSWORD=
LANGFUSE_SALT=
NEXTAUTH_SECRET=
ENCRYPTION_KEY=

IMPORTANT : g√©n√©rez des valeurs al√©atoires et s√©curis√©es pour tous les secrets. N‚Äôutilisez jamais les exemples en production.

Si vous d√©ployez en production avec Caddy, d√©commentez et configurez :

############
# Configuration Caddy
############
N8N_HOSTNAME=n8n.votre-domaine.com
WEBUI_HOSTNAME=openwebui.votre-domaine.com
FLOWISE_HOSTNAME=flowise.votre-domaine.com
SUPABASE_HOSTNAME=supabase.votre-domaine.com
OLLAMA_HOSTNAME=ollama.votre-domaine.com
SEARXNG_HOSTNAME=searxng.votre-domaine.com
LETSENCRYPT_EMAIL=votre-email@example.com

Le projet inclut un script start_services.py qui d√©marre Supabase et les services locaux. Utilisez le flag --profile adapt√© √† votre GPU :

Pour GPU Nvidia

python start_services.py --profile gpu-nvidia

Note : si vous n‚Äôavez jamais utilis√© votre GPU Nvidia avec Docker, suivez les instructions Ollama Docker.

Pour GPU AMD (Linux)

python start_services.py --profile gpu-amd

Pour Mac / Apple Silicon

CPU uniquement :
python start_services.py --profile cpu

Ollama local + Mac :
python start_services.py --profile none

Pour Mac avec Ollama local (hors Docker)
Dans votre docker-compose.yml, sous la cl√© x-n8n :

x-n8n: &service-n8n
  # ... autres configurations ...
  environment:
    # ... autres variables ...
    - OLLAMA_HOST=host.docker.internal:11434

Ensuite, une fois l‚Äôinterface accessible (http://localhost:5678/) :

Rendez-vous sur http://localhost:5678/home/credentials

Cliquez sur Local Ollama service

Modifiez l‚ÄôURL de base en http://host.docker.internal:11434/

Pour tous les autres cas :

python start_services.py --profile cpu

D√©ploiement dans le Cloud
Pr√©requis
Machine Linux (Ubuntu recommand√©) avec Nano, Git et Docker install√©s

√âtapes suppl√©mentaires
Ouvrez les ports requis :

sudo ufw enable
sudo ufw allow 80 443 3000 3001 3002 5678 8000 8080 11434
sudo ufw reload

Configurez vos enregistrements DNS de type A pour chaque sous-domaine (n8n, Open WebUI, etc.) vers l‚ÄôIP de votre serveur.

D√©marrage rapide et utilisation
Ouvrez http://localhost:5678/ dans votre navigateur pour configurer n8n (une seule fois).

Chargez le workflow fourni : http://localhost:5678/workflow/vTN9y2dLXqTiDfPT

Cr√©ez les identifiants :

Ollama URL : http://ollama:11434

Postgres (Supabase) : h√¥te db, identifiants issus de .env

Qdrant URL : http://qdrant:6333 (API key libre)

Google Drive : suivez le guide n8n

Cliquez sur Test workflow pour lancer l‚Äôex√©cution.

√Ä la premi√®re ex√©cution, patientez le temps du t√©l√©chargement de Llama3.1.

Activez le workflow et copiez l‚ÄôURL du webhook Production.

Ouvrez http://localhost:3000/ pour configurer Open WebUI.

Dans Workspace ‚Üí Functions, cliquez sur Add Function :

Saisissez un nom et une description

Collez le code de n8n_pipe.py

Dans les param√®tres, d√©finissez n8n_url sur l‚ÄôURL du webhook copi√©, puis activez la fonction.

Vous avez d√©sormais acc√®s √† plus de 400 int√©grations et √† des n≈ìuds IA (Agent, Text classifier, Information Extractor). Pour rester en local, utilisez le n≈ìud Ollama et Qdrant.


Mise √† jour

# Arr√™ter tous les services
docker compose -p localai --profile <votre-profil> \
  -f docker-compose.yml -f supabase/docker/docker-compose.yml down

# T√©l√©charger les derni√®res images
docker compose -p localai --profile <votre-profil> \
  -f docker-compose.yml -f supabase/docker/docker-compose.yml pull

# Relancer avec votre profil
python start_services.py --profile <votre-profil>

D√©pannage
Probl√®mes Supabase
supabase-pooler red√©marre en boucle : suivez cette issue.

supabase-analytics n‚Äôarrive pas √† d√©marrer : supprimez le dossier supabase/docker/volumes/db/data.

Docker Desktop : activez ¬´ Expose daemon on tcp://localhost:2375 without TLS ¬ª.

Service Supabase indisponible : √©vitez le caract√®re @ dans le mot de passe Postgres.

Probl√®mes GPU
Windows GPU : dans Docker Desktop, activez le backend WSL 2 et consultez la doc GPU Docker.

Linux GPU : suivez les instructions Ollama Docker.

Lectures recommand√©es
Agents IA pour d√©veloppeurs : th√©orie et pratique (n8n)

Tutoriel : construire un workflow IA dans n8n

Concepts Langchain dans n8n

Comparatif : Agents vs Chains

Qu‚Äôest-ce qu‚Äôune base vectorielle ?

üé• Vid√©o de pr√©sentation
Guide de Cole pour le Local AI Starter Kit

üõçÔ∏è Galerie de templates IA
https://n8n.io/workflows/?categories=AI

AI Agent Chat

AI chat avec source de donn√©es

Chat avec OpenAI Assistant (m√©moire)

LLM open-source (HuggingFace)

Chat avec PDF (citation)

Agent de scraping

Conseils & astuces
Acc√®s aux fichiers locaux
Le dossier partag√© mont√© dans le conteneur n8n (/data/shared) permet de :

Lire/√©crire des fichiers (Read/Write Files from Disk)

D√©clencher √† partir de fichiers locaux (Local File Trigger)

Ex√©cuter des commandes (Execute Command)

üìú Licence
Ce projet (initialement cr√©√© par l‚Äô√©quipe n8n) est sous licence Apache 2.0.
Voir le fichier LICENSE pour plus de d√©tails.
