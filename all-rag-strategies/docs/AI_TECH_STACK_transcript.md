# AI Tech Stack â€” Transcript

at this point I have basically already covered all the technologies that I use throughout my content but what I have never done before is created a single video going over my entire tech stack and why I use the tools that I use and so that my friend is what I have for you today so if you're looking to build any software going into 2026 listen up cuz I got some really solid recommendations for you my tech stack is AI first cuz I really don't see another way to do it these days and I've been using pretty much the same tech stack except for some of the newer technologies for over a year now so it doesn't change often it is very stable and I consider this a good thing in fact before we even get into all of the individual tools here the biggest recommendation that I have for you is to find what works well for you and just stick with it now you still want to be flexible like sometimes there are tools outside of your normal stack that will solve a problem better you want to do that but generally just finding what works well for you the mantra that I always have is capabilities over tools and what that means for this video specifically is instead of this being like oh let's dive into the nitty-g gritties of every single tool here and obsess over them I more just want this to be a resource for you if there's ever a certain technology as a part of your stack that you're unsure of then just take my recommendation so you can get right into solving the actual problem building the actual software instead of worrying about the tools cuz in the end problems come first and so that's what I have for you in this video all right so here's how I have things laid out for you in categories i'm going to start by going over my core infrastructure the technologies that apply to basically anything that I am building and then I have things split into the different types of applications so the tech I use for basically all my AI agents what I use for rag agents specifically then agents based around web automations then I'll get into my full stack development tech stack and then I'll end things off with some deployment and infrastructure and then also self-hosting options for things I haven't already covered because local AI is a big part of what I do as well and for each of the technologies I'm also going to be covering alternatives like a couple of alternatives for Postgress for example i think this will really help frame how the technology fits into my stack and also just show you that I'm not just trying to give you one tool as the end- all beall i want to talk about some alternatives and how that relates to what I end up using and why I use that over some of the other options out there

---

## Core infrastructure

the foundation that powers everything not that I use these technologies all of the time but they apply to every type of software that I build

### Database
- for my database most of the time I am using Postgress and I'm either using it through Neon or Superbase generally so they are both platforms that host Postgress under the hood and I've been using Superbase for longer but experimenting with Neon more recently especially because of their scalability they both look very similar and operate similarly like this is my dashboard for Superbase this is the one for neon both running Postgress under the hood and then I also have Postgress su through superbase as a part of the local AI package so a lot of the stack that I cover today that's open source I have that as a part of the single package for you to run everything on your own hardware and so yeah Postgress is fantastic and for anything that's open source going forward by the way I'll have this check mark and oss label right here so I don't have to just like say it every single time because that is one of the benefits of a lot of the things that I'm covering in this video as far as alternatives we have MongoDB and fire store and so just speaking to a couple of NoSQL alternatives And I used to be a big fire store guy even using MongoDB a bit maybe like four or five years ago and I switched a lot more to SQL recently because it's definitely the industry standard for building AI agents and I feel like LLMs understand SQL a lot more than writing queries for NoSQL like fire store and MongoDB and because of pricing and scaling I just know a lot of people including myself that have switched to SQL and Postgress is just the standard for that so that is my database that powers most things we'll see Postgress a lot going forward as well

### Caching
- then for caching specifically I use Reddus it is blazing fast and then also we have Valky as an open- source alternative that's one of the things I have in the local AI package and it's completely compatible so I can use Reddus if I want something hosted and convenient but then always switch to something local if I want and oh did I mention that it is blazing fast so great tool for caching

### AI coding assistant
- then for my AI coding assistant honestly probably the tool that I have open most up on my computer is Claude Code and I'm always using it with Archon which is my open source project i'll link to a video right here it's knowledge and task management for AI coding assistance and so this enhances my AI coding experience a lot but Cloud Code is my primary driver there are some fantastic alternatives like cursor with their new 2.0 codeex is definitely catching up to cloud code but right now it is generally considered the best AI coding assistant though it is a bit pricier and has some rate limit issues that people are dealing with right now but it is still my daily driver and just the features they have around slash commands and sub agents and the new Claude skills is very very cool and so I'm using Archon along with it for everything that I'm creating like literally all of the different types of software that you'll see in the rest of this video I'm building it with the help of Claude Code

### No-code prototyping
- and as much as I'm using AI coding assistants all of the time I still love using no code tools to help me prototype what I'm building especially when I'm creating AI agents and there are some good alternatives like Langflow and Flowwise i even covered Flowwise on my channel one time and it's also in the local AI package but N8N for me is where it's at and I feel like it's going to be like that long term it is a phenomenal platform that I've covered so much on my channel building things like rag agents like you're looking at right here and so I'll use it as a tool to quickly prototype ideas validate the tools I'm giving my agents and system prompts things like that and then I'll usually move to a coded solution once I'm confident in my prototype and and is great because of all their app integrations it's very AI focused they're building in features all the time to help with creating agents and of course it's open sourced and self-hostable and again I'm not going to mention that as a benefit going forward but you'll see the label right here

- as we move on from core infrastructure the last thing I want to say as I'm going through everything here is that obviously I'm not listing every alternative but I just want to list a couple to help frame the conversation here and then also for some tools I don't actually have alternatives because I just haven't tried something comparable or I just genuinely don't think there's another tool worth looking into and so with that that'll take us into the next category here for the tools that I'm using for all of the AI agents that I create

---

## Tools for all AI agents

- now for my AI agent framework right now I'm pretty much exclusively using Pideantic AI which people ask me why all of the time because there are so many good alternatives even using no framework at all a lot of people like just using raw LLM calls and I respect that as well because a lot of these frameworks are what I like to call an abstraction distraction you end up fighting the framework more than it's actually helping you because it's hard to really customize things once you get into the nitty-gritty of building your agents but I feel like I don't have that issue with Pyantic AI it's a lot easier for me to build agents than just raw LLM calls especially when I'm swapping between different providers but I still feel like I have all the flexibility and control that I need as if I was doing raw LLM calls plus Pyantic AI is constantly supporting new protocols like MCPU when that first came out A2A for UI event streams we have AGUI the new Versell uh data streaming protocol and so it's easy to just build on top of the latest in in AI thanks to Pyantic AI

- and then for multi- aents specifically I love using Langraph and a lot of times I'll be actually using these two together so Pantic AI is how I create my individual agents and then Langraphph is how I connect them together in more complex workflows when the use case actually calls for multi-agent i don't want to overengineer and always do multi-agent so I'm only using lang graph for the more complex use cases but it is a lifesaver it is so easy to manage state human in the loop for more complex workflows routing between my different agents graph persistence they have a user interface to help us visualize our graphs langraph is awesome and there are some alternatives like a lot of people love using crew AI for building multi-agent pidantic AI graphs and so I could actually use the same framework to build multi- aent that I use for building single but I feel like langraph is just the most mature handling some of the core components of these more complex workflows like human in the loop for example

- now frameworks like pideantic AI make it very easy to add tools into our AI agents but what they don't handle is agent authorization and tool security that is where arcade comes in and so for example if your AI agent needs to get permission to access a user's account like to view emails in their Gmail send a message on their behalf in Slack the agent needs some sort of way to authorize with those accounts and that is where Arcade comes in that is a very hard thing to engineer without a tool like Arcade and I don't actually know of any alternatives that do it the way that they do plus they've recently released an MCP server SDK so we can build secure MCP servers directly with Arcade leveraging their tool sets leveraging their tool authorization right within the platform and so I'll actually show you the docs really quickly they have this entire guide now how we can build our own MCP server so it's basically building MCP servers like fast MCP if you've ever built a server in Python with that before but now we can bake in the arcade tool authorization right into these servers and so this is a Reddit example we got different tools to like get subreddit posts for example get post content but then for the ones that need authorization specifically we just add this decorator here so that when we call this tool it's going to send a user through a ooth flow so the agent has access to their Reddit account to you know for example get their username or submit a text on their behalf so our agent can do things for individual users now that's the kind of authorization that's very hard to engineer for without something like Arcade and in a video on my channel that I'll link to right here I did a demonstration incorporating Arcade into Langraph so I have this AI agent where for example I can say "What emails do I have from today?" And take a look at this the agent doesn't have access to my account yet and so it sends me this link so I click on this link to go through an ooth flow so I go through this I sign into Arcade now the AI agent going forward is going to have access to my account to do things on my behalf but also just with the permissions that I set up in the application so it's also very secure very granular and so authorization successful now if I go back to my agent it's going to list out my emails and you can do this whole OOTH flow directly through MCP as well i just wanted to show you the example from my longer video in case you're interested in checking that out

- but yeah the last thing that is core to all of my AI agents is Langfuse this is for agent observability and you cannot skip this step it is so important especially when your agents are running in production to be able to monitor them so for every single run you can see the token usage the total cost the latency the tool calls that your agents are making and with their Langraph integration so just like Arcade they integrate with Langraph all my tools work very well together you can also see the different agents in a multi- aent system and the decisions that are made like what what agents are being routed to in this execution so without something like this there's no way to set up evaluations AB test or system prompts for example when you're serious about making your agents reliable you need a tool like this for observability there are some other good options as well like Langmith and Helone for example but I find Langfuse to be the most featurerich and Langmith is very popular but also not 100% open- source and self-hostable and so that's one of the things I love about Langfuse it's another one of those technologies that I have in the local AI package as well

---

## Tools for RAG agents

- next up I want to talk about all the tools that I'm using for rag agents specifically so I just covered what I use for all AI agents so these four do apply in general to rag agents too but now these are tools specifically for data extraction we have data storage knowledge graphs and then evalarch I would include in rag and so first of all for data extraction you don't always need a framework but for more complex documents specifically like PDFs with diagrams Excel files it's helpful to have something out of the box like dockling to help and there are some good alternatives like llama index has a lot it's a agent framework specifically for rag we've got unstructured but man dockling has been crushing it for me pretty recently this is actually one of the newest additions to my tech stack a lot of before I've just been kind of doing manual extraction without a framework but this has just made it so much easier for me and it's easy to use self-hosted models and it's completely open- source unlike a lot of data extraction tools

- and then for website data specifically I use crawl for AAI it's very fast and efficient it automatically cleans junk they have LLM integrations you can send it into a website and use an LLM and just kind of like ask it for what text you want to extract specifically so it's very very featurerich and so think of it this way and I'll go to the documentation here if you are working with files use dockling if you are working with websites use crawl for AAI that's my decision process these are the two core tools that I use for data extraction specifically

### Storage & Vector
- and then for storage I love using Postgress so not only is it my regular database that powers all my applications but thanks to PG vector I can basically use Postgress as a vector database now it's not a dedicated vector database like something like Quadrant or Pine Cone these are alternatives that are going to be faster so it is a trade-off when you have a dedicated vector database they are faster but the reason I like using Postgress is a lot of my rag strategies they need a regular database as well like they just need a SQL database to store document data or to store user data as it relates to rag and so that kind of thing I really like using Postgress for and it also scales extremely well

### Long-term memory
- and then for long-term memory specifically the reason I have this here in my rag agent tools is because long-term memory is just a form of rag it's an implementation of rag and I love using mem cover this on my channel as well by the way I'm going to keep linking to videos that relate to these technologies cuz like I said I have most covered most of them in my content already but I love Mem Zero because it integrates with really any database that I want so I can use Mem directly with PG Vector which is very nice they're right next to each other and they also integrate together um and it's just super easy to add into any AI agent so there are some other great long-term memory frameworks let's example that I'm not putting here because it's actually more of an AI agent framework focused on long-term memory but mem zero we can incorporate this with any AI agent like I can build my agent with pideantic AI and then I can search for memories inject them into the system prompt and then extract memories after so I can kind of just sandwich my agent no matter how I build it with mem zero and then Zep is an alternative uh that is not open source like me zero which is why I generally prefer it so that is long-term memory

### Knowledge graphs
- now I want to get on to the fun part here with knowledge graphs a lot of content I've done on knowledge graphs recently and so for knowledge graphs there's really two parts we have the graph database and then generally you're going to use some kind of library to help you insert data and search through data in your knowledge graphs and so for the graph database what I like to call the engine I love using Neo4j they have a really beautiful UI it's easy to query through our data and work with it and then also one of the great things with Neo4j is that it is supported by most knowledger graph libraries like a couple that I'm going to talk about in a second here it's also very high-speed and very very scalable now the licensing is not ideal for Neo4j uh even though it is open source so when you get into commercial use it's something to look into if you're interested in Neo4j there are some great alternatives as well like memaph and folklore DB

- and now for the library to help us insert data into Neoforj and search our knowledge graphs here i love using graffiti and so cover this on my channel as well it's a very intelligent entity and relationship extraction and so going back to our graph here we have these different entities and then we also have information or metadata for how they relate together and obviously unless we have a super specific use case where we can somehow extract this deterministically we need to use a large language model to extract entities and relationships from raw text and that is what graffiti helps us with so this is where we store the data this is how we get the data in the format to store and how we search it as well light rag is another alternative I've covered on my channel quite a bit uh but light rag is more like vector database and knowledge graph graffiti is focused just on knowledge graphs and I like to have them separated and so I've even done implementations on my channel where I have a gentic rag i give an AI agent the ability to search both a vector database and a knowledge graph depending on the user question very powerful stuff

### Evaluation
- for evaluation i love using regos it is I'm not sure if I'm pronouncing that regos i don't know if I'm pronouncing it right but I love using it for setting up eval pipelines and so specialized metrics for rag like faithfulness relevance very very powerful a tool like Langfuse we can use for agent evals overall but that's more around like the tool calling of agents but when you want to look at things like these metrics specifically that's when Raos is fantastic and it also helps with automated test data set generation and works with any LLM provider

### Web search
- and then last but not least here web search you could kind of consider this a part of web automation agents but also this just was the last spot I had to fill in this section and it is a part of rag right like retriegal augmented generation it's not just searching your own knowledge it's just searching the general knowledge the wider web and that's what Brave helps me with and perplexity is another great tool for this as well i definitely use both uh this is more of like a highle faster implementation and then perplexity is more in detail but slower for AI agents to leverage and Brave is privacy focused and no tracking so it's not stupid like our AI browsers like Atlas right now uh independent index so no Google proxy and they also have AI search functionality built in that I leverage quite a bit

---

## Web automation agents

- now the difference between these two is when I do the data extraction like with crawl for the web here it's when I'm pulling documentation ahead of time to store in my knowledge base but for web automation this is when I'm giving tools to my AI agent to pull information from the web live and so sometimes I'll give my agents a crawl for AI tool so it can extract text from a specific URL live instead of as a part of my rag pipeline firecrawl is another good alternative for this but yeah crawl for AAI is open source fast and so featurerich and then the one thing it doesn't handle well is social platforms specifically like LinkedIn X Instagram agents that have to work with that I'll often use something like Ampify or Bright Data

- and then for simpler browser automations when I just want to like quickly code up some automations for web testing or using the Playright MCP server for my AI coding assistant to be able to visually validate website changes it makes i love using Playright and then Puppeteer and Selenium are good options as well well I used Selenium for years and years and years but have switched to Playright within the last year here and so yeah it's great multi browser support it is in my mind the deterministic web automation king like I said the Playright MCP that's a golden nugget it's really awesome when you're using coding assistants to work on your frontends

- and then last but certainly not least I saved browserbase when I want to take these things further and have an agent control a browser for me live that is when I will use browserbase and it is a beautiful tool i haven't found anything like it i know there are some alternatives but like man this is the king for me managed infrastructure all sessions are recorded and stored so you have the stage hand MCP server where you just with natural language describer request it'll spin up this session for you that has anti-bot detection it's very secure it's managed infrastructure it'll go and navigate websites to find the information that you need and then you can go back later and actually review the session so I can play the session live to see what it did to extract the information it needed to based on my request so awesome platform and then also they have director which by the way stage hand and director are all built on top of playright and they've extended to work with some of the other tools like puppeteer and selenium so it's deterministic web automation but putting AI on top of it for browser control so with director for example I could say get me the latest price for the body fortress protein powder on Amazon just any kind of web task that I have for it and it'll go and navigate through everything it'll show me step by step what it did even screenshots to go along with it very very cool gives me the final response at the end but then the other thing I can do is I can go to the artifacts tab here i can go to the code and take a look at this i can actually copy the code directly so I can bring this into my own codebase my own automations that I have for testing or just web automations in general it is an awesome platform and again it's just using these tools under the hood but applying AI on top so an example of one how I'm using these tools that work well together and then also how I have things in my stack that's very AI first because like I said I don't see another way these days it's just so powerful the tools that we have access to

---

## Full stack development

- at this point I've covered basically everything for AI agents specifically because that is where I focus most of my time but I still do a lot of full stack development as well especially because our AI agents still need backends and frontends for a lot of our implementations like for example this is the front-end application that I have built for the Dynamis AI agent mastery course using pretty much all of the technologies that I'll cover with you in the next couple of minutes here so a chat GBT like interface here to interact with some multi- aent workflows that I have behind the scenes like this one that you're looking at right here that helps us with SEO research and social media research competitor research all in parallel so very cool implementation this is one of the many implementations that I do in the Dynamis AI agent Mastery course but yeah I built this pretty much with all of the tech that we're seeing right here

### APIs & Backends
- and so for my APIs I always use fast API because I like building my AI agents with Python and so I want to have my API framework be Python as well flask is also good but I think that fast API is more featurerich and then if you do want to build your agents and backends with TypeScript then I think Express is fantastic

### Database & Auth
- and then for my database Postgress like usual not going to cover this again super super standard and then for simpler authentication Superbase has been crushing it for me for a while now and I know Neon has this as well but specifically when I've been using Neon I haven't been using it for authentication so yeah Superbase is my go-to and then also integrates with Ozero when you need to have more enterprise authentication but you still want to be using Superbase as your database you can still take advantage of like the roadle security that it offers but when you have more enterprise needs for uh MFA universal login with more providers enterprise SSO with SAML or Active Directory that's when Ozero is crucial and there are some other platforms that are fantastic as well like Clerk and Octa i'm going to be fully transparent here ozero like enterprise authentication is the one thing out of pretty much everything in this tech stack where I I picked Ozero initially and I haven't really tried these other ones i just know that they are considered to be very fantastic as well but this is the one that I've been using for the most part if I'm not just sticking with simple superbase authentication

### Frontend
- and then for my front-end library I am just always using React now it React is just so simple it's what all of your AI coding assistants will prefer to use and your front-end builders like lovable that we'll talk about in a second here and then Vit to go along with that for the build tool and so React plus Vit just leads to very snappy quick to build lightweight applications frontends for things like your agents i have worked with Nex.js a lot in the past and I still have a lot of respect for it but especially cuz things have changed so much between recent versions like Nex.js 12 the 13 14 15 it keeps like breaking my code so I've kind of moved away from Nex.js to be honest even though I I like it a lot view is another good option as well

### UI libraries & builders
- and then for my component library and styling library a lot of good options but Shad CN has been great for me um and then Tailwind CSS for styling as well and so yeah not much to say on those those are like pretty standard these days and they work fantastic and then I love using claude code for most of my AIdriven coding but the one thing it doesn't do the best for me is creating beautiful UIs and so I like taking advantage of an agentic front-end builder like lovable for example that has a lot of system prompting for their agent builder behind the scenes on how to actually create UIs that are beautiful so the kind of thing I could just do in Claude Code or my standard coding assistant but I like using tools like lovable or bolt new or our very own bolt.diy DIY if you want a completely open source version uh for an agentic front-end builder but yeah Lovable just has great integrations and their new agent mode that's been crushing it for me recently uh and then the last UI library that I want to cover is Streamlit and so this is like the easiest user interfaces you could possibly make because you build it directly in Python and so I love using this as a tool to prototype things like my AI agent so I still have a nice user interface to chat with the agent but I don't have to build a full-on React application yet and so just defining my UIs and Python code not having to create a backend front-end kind of connection and things like that

### Process & Monitoring
- and so my general process for building an AI agent you can kind of tell from my tech stack here is I'll start by prototyping the agent in N8N i'll move it to code but still have a simple front end with Streamlit and then when I'm ready I will create a full React application with something like Lovable and then that's where I'm really starting to build my MVP and then for app monitoring and analytics I like using Sentry post Hog is really popular as well i've used the this and Google Analytics before uh but yeah Sentry just has really great real-time analytics for like everything that you need and there's working on native AI features which is why I'm starting to lean towards it over other tools and just all the integrations they have

### Payments
- and then for payments I've always used Stripe and so I guess this is kind of similar to Enterprise O with Ozero where I know of these other options like lemon squeezy and paddle but um yeah Stripe just has the best developer experience from what I know and fantastic documentation

---

## Deployment & Infrastructure

everything for getting the code into production so deployment platforms things like CI/CD and the testing and AI code review there

### Deployment platforms
- first up deployment platforms and so render is the simplest one that I lean on a lot because of its simplicity there are a lot of other platform as a service options like fly.io or netlifi the thing I like the most about render and this is pretty specific otherwise these are pretty similar but you can define your infrastructure as code in YAML for render to make your deployments automated it's just so easy to use also git based deployments so that you push to a repo to a certain branch it'll automatically update like your front end for example it's free to host frontends and scaling pretty much infinitely with their CDN background workers and cron jobs the one thing I'll say is that if I have a requirement to go more enterprise for a certain client for example because of SLAs's or compliance whatever and I want to use the you know bare bones like AWS or Google cloud I do like using Google cloud uh GCP specifically especially with their serverless functions um it's really really nice you don't really have true serverless this when you're using a platform as a service like render

### GPU workloads
- and then when I am running uh GPUheavy workloads like running things with local AI that I want to deploy in the cloud I love using runpod there are some alternatives like tensor doc if you want something really really cheap but not quite as reliable uh lambda labs as well but yeah runpod as far as like super reliable GPU hosting it is the cheapest from what I know and then also spot instances for lower cost and so yeah if you're willing to sacrifice some reliability for cost then you get that as well and it's just like you get your GPUs instantly there's no queue it's always going to be available for you at least from my experience fantastic platform

### VMs & hosting
- and then the last thing for uh virtual machines specifically so like render they manage the infrastructure for you if you want to own the machine and manage the machine the digital ocean is what I use so for example hosting the local AI package in the cloud I do that on digital ocean i know a lot of others who like using Hostinger with their KVM offering or Hner hner is very affordable as well so yeah Digital Ocean is a bit of a premium over these two but very reliable and it's got predictable pricing i also love the integrations they're starting to add into AI like they have the app platform for managed databases and hosting local LMS they have things for rag built into the platform so very featurerich

### Containerization
- and then as far as containerization which is how I'm deploying all of my applications except for some frontends is I'm using a Docker so it really is just like the industry standard for deploying applications creating isolated environments to run things uh to fix the works on my machine problem right like sometimes you host an application on your machine or on some VM in the cloud and you have dependencies there that uh you're relying on those and so when you go to another machine or another cloud provider it breaks but if you have everything in Docker then it's guaranteed if it works in one place as long as you have the container up and running it will work in another place and I know a lot of people like using Podman because of some licensing with Docker that isn't necessarily ideal so yeah Podman is a good alternative um I've used this a lot before actually my old company that I was working for before I was doing everything with you know YouTube and Dynamus we moved from Docker to Podman because of the licensing cost it was like hundreds of thousands of dollars for the company um but then we ended up moving back from Podman to Docker and uh this was actually after I left the company so I just heard from my team after so I know that Podman isn't quite as robust as Docker but it is still a good alternative

### CI/CD & testing
- and then going down to the bottom here for CI/CD I am always using GitHub actions it's so simple integrated right within my repositories that I also always have in GitHub it's free for public repos but uh very generous pricing for private repos um and a huge marketplace of actions and and by the way it's like it is free for the most part for private repos i think it's just when you're like adding team members i I don't know the pricing exactly but it's very very generous and yeah just super easy to get it set up and AI coding assistants by the way are very good at creating the YAML for your GitHub action workflows they can help you automate things like testing in your GitHub repo so for Python testing I'm using Piest typescript I'm using Jest like super super standard both are really easy to use and help with things like mocking and fixtures that are really important to make your testing reliable um and just doing it the right way

### AI code review
- and then last but not least in our section here for deployment and infrastructure is AI code review and I love using code rabbit and so for example within Archon my open- source tool that I showed earlier for knowledge and task management every single poll request that we have coming into Archon gets automatically reviewed by Code Rabbit and man is it thorough honestly sometimes it's a bit too thorough but there are ways to customize it within the Code Rabbit dashboard and the biggest reason that I love using it is because it is completely free for open- source repositories like Archon so if it's a public repo you can get like just infinite AI code reviews that are very detailed completely for free and it also includes uh security vulnerability detection

---

## Open source & local AI tools

- and then really quickly just for some of the open source things that that didn't fit into another section I love using open web UI as a local LLM chat platform so it's like chat GBT but it's running completely locally you can add in your own custom agents through functions and pipelines it's just really featurerich they got things like rag built right into it anything LLM is another good option and then for completely local web search I like using CRXNG by the way all of these I have them incorporated into the local AI package including Olama for serving local LLMs pretty much any large language model that is open source that you want to run locally uh you can do it with Olama and then VLM and light LLM are nice as well i just find Olama to be the easiest to use and it autoleverages multiple GPUs so you don't have to set up fancy config and stuff but there also are some things you can configure for quantization like they always have all the different quantized versions of the models you can configure the context limit for your LLMs to protect your memory very very nice

### HTTPS / TLS
- and then for uh HTTPS and a TLS platform so that I can have my domains for my things that I'm running myself i love using Caddy and then traffic and engineext are great i find Caddyy to be the simplest which is why I included it here but yeah these are great as well and I know some people in the Dynamus community specifically that are gung-ho about using traffic so yeah it doesn't matter a ton i just find Caddy to be the simplest

---

## Closing

so there you go that is my entire agentic engineering tech stack and remember to just take these as recommendations the most important thing is for you to find what works for you and just generally stick with it be willing to be adaptable as well because remember the most important thing is to be a problem solver not someone who's becoming an expert at specific tools and so just use these recommendations as a way to fill holes in your tech stack if you're not sure what to use

- the last thing that I want to say is that naturally as a content creator I end up working with some of these teams for the tools that I have chosen to use myself browserbased and arcade are two of those and so I did reach out to them for this video especially because there's a lot of different ways to leverage their tools i just wanted to find how they wanted me to showcase their tools and so thanks to them for working with me on that and so with that if you appreciated this video you're looking forward to more things on AI coding and building AI agents I would really appreciate a like and a subscribe and with that I will see you in the next video